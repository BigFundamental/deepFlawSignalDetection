{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named script.feature_extractor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mImportError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a8ffa80b1b8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'load_ext autoreload'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extractor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFeatureExtractor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignal_manager\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSignalMgr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFilter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named script.feature_extractor"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "from script.feature_extractor import FeatureExtractor\n",
    "from script.classifier import Classifier\n",
    "from script.signal_manager import SignalMgr\n",
    "from script.filter import Filter\n",
    "from script.data_reader import DataReader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import xgboost as xgb\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "INVALID_SLIGHT_DATA_FPATH='/Volumes/workspace/projects/signal_classification/data/特殊次品样本/斜角_轻微.20190515/'\n",
    "INVALID_BAD_DATA_FPATH='/Volumes/workspace/projects/signal_classification/data/特殊次品样本/斜角_严重.20190515/'\n",
    "FULL_DATA_FAPTH='/Volumes/workspace/projects/signal_classification/data/1005_0830重新标注文件_Data._20180609.0830'\n",
    "MISS_LABEL_NORMAL_FPATH='/Users/changkong/project/signal_classification/data/20190623标记/20190623NEW'  # 误分的正样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_reader = DataReader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_name</th>\n",
       "      <th>channel_id</th>\n",
       "      <th>case_path</th>\n",
       "      <th>expect_result</th>\n",
       "      <th>reason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20180325_090536</td>\n",
       "      <td>1</td>\n",
       "      <td>/Volumes/workspace/projects/signal_classificat...</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20180325_090637</td>\n",
       "      <td>1</td>\n",
       "      <td>/Volumes/workspace/projects/signal_classificat...</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20180325_091016</td>\n",
       "      <td>1</td>\n",
       "      <td>/Volumes/workspace/projects/signal_classificat...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20180325_091047</td>\n",
       "      <td>1</td>\n",
       "      <td>/Volumes/workspace/projects/signal_classificat...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20180325_091103</td>\n",
       "      <td>1</td>\n",
       "      <td>/Volumes/workspace/projects/signal_classificat...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         case_name  channel_id  \\\n",
       "0  20180325_090536           1   \n",
       "1  20180325_090637           1   \n",
       "2  20180325_091016           1   \n",
       "3  20180325_091047           1   \n",
       "4  20180325_091103           1   \n",
       "\n",
       "                                           case_path  expect_result  reason  \n",
       "0  /Volumes/workspace/projects/signal_classificat...              0      -1  \n",
       "1  /Volumes/workspace/projects/signal_classificat...              1       9  \n",
       "2  /Volumes/workspace/projects/signal_classificat...              1       5  \n",
       "3  /Volumes/workspace/projects/signal_classificat...              1       5  \n",
       "4  /Volumes/workspace/projects/signal_classificat...              1       5  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_df = data_reader.create_single_index(FULL_DATA_FAPTH+'/'+'result.csv').drop(labels='sys_result', axis=1)\n",
    "norm_df[norm_df.reason==4].describe()\n",
    "norm_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_name</th>\n",
       "      <th>channel_id</th>\n",
       "      <th>case_path</th>\n",
       "      <th>expect_result</th>\n",
       "      <th>reason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20190515_204352497</td>\n",
       "      <td>1</td>\n",
       "      <td>/Volumes/workspace/projects/signal_classificat...</td>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20190515_204352497</td>\n",
       "      <td>2</td>\n",
       "      <td>/Volumes/workspace/projects/signal_classificat...</td>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20190515_204352497</td>\n",
       "      <td>3</td>\n",
       "      <td>/Volumes/workspace/projects/signal_classificat...</td>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20190515_204352497</td>\n",
       "      <td>4</td>\n",
       "      <td>/Volumes/workspace/projects/signal_classificat...</td>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20190515_204352497</td>\n",
       "      <td>5</td>\n",
       "      <td>/Volumes/workspace/projects/signal_classificat...</td>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            case_name  channel_id  \\\n",
       "0  20190515_204352497           1   \n",
       "1  20190515_204352497           2   \n",
       "2  20190515_204352497           3   \n",
       "3  20190515_204352497           4   \n",
       "4  20190515_204352497           5   \n",
       "\n",
       "                                           case_path  expect_result  reason  \n",
       "0  /Volumes/workspace/projects/signal_classificat...              1      61  \n",
       "1  /Volumes/workspace/projects/signal_classificat...              1      61  \n",
       "2  /Volumes/workspace/projects/signal_classificat...              1      61  \n",
       "3  /Volumes/workspace/projects/signal_classificat...              1      61  \n",
       "4  /Volumes/workspace/projects/signal_classificat...              1      61  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "invalid_slight_df = data_reader.get_signal_list(INVALID_SLIGHT_DATA_FPATH).drop(labels=['sys_result'], axis=1)\n",
    "invalid_slight_df['expect_result'] = 1\n",
    "invalid_slight_df['reason'] = 61\n",
    "invalid_slight_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_name</th>\n",
       "      <th>channel_id</th>\n",
       "      <th>case_path</th>\n",
       "      <th>expect_result</th>\n",
       "      <th>reason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20190515_203431979</td>\n",
       "      <td>1</td>\n",
       "      <td>/Volumes/workspace/projects/signal_classificat...</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20190515_203431979</td>\n",
       "      <td>2</td>\n",
       "      <td>/Volumes/workspace/projects/signal_classificat...</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20190515_203431979</td>\n",
       "      <td>3</td>\n",
       "      <td>/Volumes/workspace/projects/signal_classificat...</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20190515_203431979</td>\n",
       "      <td>4</td>\n",
       "      <td>/Volumes/workspace/projects/signal_classificat...</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20190515_203431979</td>\n",
       "      <td>5</td>\n",
       "      <td>/Volumes/workspace/projects/signal_classificat...</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            case_name  channel_id  \\\n",
       "0  20190515_203431979           1   \n",
       "1  20190515_203431979           2   \n",
       "2  20190515_203431979           3   \n",
       "3  20190515_203431979           4   \n",
       "4  20190515_203431979           5   \n",
       "\n",
       "                                           case_path  expect_result  reason  \n",
       "0  /Volumes/workspace/projects/signal_classificat...              1      62  \n",
       "1  /Volumes/workspace/projects/signal_classificat...              1      62  \n",
       "2  /Volumes/workspace/projects/signal_classificat...              1      62  \n",
       "3  /Volumes/workspace/projects/signal_classificat...              1      62  \n",
       "4  /Volumes/workspace/projects/signal_classificat...              1      62  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "invalid_bad_df = data_reader.get_signal_list(INVALID_BAD_DATA_FPATH).drop(labels=['sys_result'], axis=1)\n",
    "invalid_bad_df['expect_result'] = 1\n",
    "invalid_bad_df['reason'] = 62\n",
    "invalid_bad_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sigMgr = SignalMgr()\n",
    "# feature = sigMgr.get_features(path1, request_param={'skip_row':[1], 'model_path':['train']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 开始进行模型迭代和训练, 整合训练数据和测试数据\n",
    "\n",
    "def data_prepare(train_path, test_path, eval_path):\n",
    "    # 预留长短波形的数据用于数据的测试\n",
    "    msk = np.random.rand(len(invalid_bad_df)) < 0.8\n",
    "    invalid_bad_train_df = invalid_bad_df[msk]         #用于训练\n",
    "    invalid_bad_eval_df = invalid_bad_df[~msk]         #用于最后验证\n",
    "    \n",
    "    msk = np.random.rand(len(invalid_slight_df)) < 0.8\n",
    "    invalid_slight_train_df = invalid_slight_df[msk]\n",
    "    invalid_slight_eval_df = invalid_slight_df[~msk]\n",
    "    \n",
    "    eval_mix_df = invalid_bad_eval_df.append(invalid_slight_eval_df).reset_index(drop=True)\n",
    "    # 获取整体的训练数据\n",
    "    train_mix_df = invalid_slight_train_df.append(invalid_bad_train_df).reset_index(drop=True)\n",
    "    # 再次划分为测试集合与训练集合\n",
    "    msk = np.random.rand(len(train_mix_df)) < 0.8\n",
    "    train_df = train_mix_df[msk]\n",
    "    test_df = train_mix_df[~msk]\n",
    "\n",
    "    # pandas 写入到文件中进行缓存，用于迭代测试，避免出现每次划分数据集合auc发生变化\n",
    "    train_df.to_csv(train_path, index=False)\n",
    "    test_df.to_csv(test_path, index=False)\n",
    "    eval_mix_df.to_csv(eval_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data_prepare(\"../data/train_skew.csv\", \"../data/test_skew.csv\", \"../data/eval_skew.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_root = '../data'\n",
    "# 开始进行特征的获取\n",
    "train_tmp_df = pd.read_csv(data_root + '/' + 'train.csv')\n",
    "train_skew_df = pd.read_csv(data_root + '/' + 'train_skew.csv')\n",
    "train_df = train_tmp_df.append(train_skew_df).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/workspace/projects/signal_classification/data/特殊次品样本/长短.20190515/20190515_195250029/Channel_2.csv\n"
     ]
    }
   ],
   "source": [
    "print (train_df['case_path'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_name</th>\n",
       "      <th>sys_result</th>\n",
       "      <th>expect_result</th>\n",
       "      <th>reason</th>\n",
       "      <th>channel_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20190623_000136435</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            case_name  sys_result  expect_result  reason  channel_id\n",
       "0  20190623_000136435           1              0      -1           8"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MISSING_LABEL_DF_FPATH='/Users/changkong/project/signal_classification/data/20190623标记/result_man.csv'\n",
    "missing_classify_possible_df = data_reader.get_signal_list(MISS_LABEL_NORMAL_FPATH).drop(labels=['sys_result'], axis=1)\n",
    "label_df = pd.read_csv(MISSING_LABEL_DF_FPATH, header=None, skiprows=1, names=['case_name', 'sys_result', 'expect_result', 'reason', 'channel_id'])\n",
    "target_df = label_df[label_df.expect_result == 0].reset_index(drop=True)\n",
    "tmp = target_df[['case_name', 'channel_id']].merge(missing_classify_possible_df, on=['case_name', 'channel_id'])\n",
    "missing_classify_possible_df[missing_classify_possible_df.case_name == '20190623_000136435'].head()\n",
    "target_df[target_df.case_name == '20190623_000136435']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "missing_positive_df = tmp\n",
    "missing_positive_df['expect_result'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "msk = np.random.rand(len(missing_positive_df)) < 0.8\n",
    "missing_positive_train_df = missing_positive_df[msk]\n",
    "missing_positive_eval_df = missing_positive_df[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# missing_positive_train_df.to_csv(data_root + '/' + 'mp_train.csv', index=False)\n",
    "# missing_positive_eval_df.to_csv(data_root + '/' + 'mp_test.csv', index=False)\n",
    "mp_train_df = pd.read_csv(data_root + '/' + 'mp_train.csv')\n",
    "train_df = train_df.append(mp_train_df).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_tmp_df = pd.read_csv(data_root + '/' + 'test.csv')\n",
    "test_skew_df = pd.read_csv(data_root + '/' + 'test_skew.csv')\n",
    "test_mp_df = pd.read_csv(data_root + '/' + 'mp_test.csv')\n",
    "test_df1 = test_tmp_df.append(test_skew_df).reset_index(drop=True)\n",
    "test_df = test_df1.append(test_mp_df).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "case_name        261\n",
       "channel_id       261\n",
       "case_path        261\n",
       "expect_result    261\n",
       "dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()\n",
    "test_mp_df.head()\n",
    "mp_train_df.head()\n",
    "test_mp_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normal_cases = train_df[train_df.expect_result == 0]\n",
    "other_defect_cases = train_df[(train_df.expect_result == 1) & (train_df.reason != 6) & (train_df.reason != 61) & (train_df.reason != 62)]\n",
    "defet_cases = train_df[(train_df.reason == 6) | (train_df.reason == 61) | (train_df.reason == 62)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_features(df):\n",
    "    mu_list = []\n",
    "    delta_list = []\n",
    "    sigMgr = SignalMgr()\n",
    "    for path in df['case_path']:\n",
    "        feature = sigMgr.get_features(path, request_param={'skip_row':[1], 'model_path':['train']})\n",
    "        mu_list.append(np.mean(feature['unit_interviene_length_diff']))\n",
    "        delta_list.append(np.std(feature['unit_interviene_length_diff']))\n",
    "    return np.min(mu_list), np.max(mu_list), np.min(delta_list), np.max(delta_list), np.mean(mu_list), np.mean(delta_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 整体数据的区分很大，因此介入进行数据分析和训练\n",
    "feature_names = ['peaks_num', 'down_peaks_num', 'up_edges_num', 'down_edges_num', 'peak_edge_ratio', 'down_peak_edge_ratio',\n",
    "                 'edge_diff_10', 'edge_diff_20', 'width_diff_10', 'negative_peak_num', 'max_down_peak_point', 'inter_diff_mean', 'inter_diff_delta',\n",
    "                'skewness_mean', 'skewness_delta', 'cyclic_intense_nopeak', 'cyclic_intense_downpeak']\n",
    "\n",
    "feature_names = sorted(feature_names, reverse=True)\n",
    "                \n",
    "def features(df_full, feature_names):\n",
    "    pathes = df_full['case_path']\n",
    "    # print pathes\n",
    "    feature_set = dict()\n",
    "    for name in feature_names:\n",
    "        feature_set[name] = list()\n",
    "#     feature_set['inter_diff_mean'] = list()\n",
    "#     feature_set['inter_diff_delta'] = list()\n",
    "#     feature_set['skewness_mean'] = list()\n",
    "#     feature_set['skewness_delta'] = list()\n",
    "#     feature_set['skewness_median'] = list()\n",
    "#     feature_set['skewness_10'] = list()\n",
    "#     feature_set['skewness_20'] = list()\n",
    "#     feature_set['skewness_30'] = list()\n",
    "#     feature_set['skewness_']\n",
    "    \n",
    "    for test_case in pathes:\n",
    "        features = sigMgr.get_features(test_case, request_param={'skip_row':[1], 'model_path':['train']})\n",
    "        for name in feature_names:\n",
    "            feature_set[name].append(features[name])\n",
    "#         feature_set['inter_diff_mean'].append(np.mean(features['unit_interviene_length_diff']))\n",
    "#         feature_set['inter_diff_delta'].append(np.std(features['unit_interviene_length_diff']))\n",
    "#         skewness_list = sorted(features['unit_interviene_skewness'], reverse=True)\n",
    "#         feature_set['skewness_median'] = np.percentile(skewness_list, 50)\n",
    "#         feature_set['skewness_10'] = np.percentile(skewness_list, 90)\n",
    "#         feature_set['skewness_20'] = np.percentile(skewness_list, 80)\n",
    "#         feature_set['skewness_30'] = np.percentile(skewness_list, 70)\n",
    "#         feature_set['skewness_mean'].append(np.mean(features['unit_interviene_skewness']))\n",
    "#         feature_set['skewness_delta'].append(np.std(features['unit_interviene_skewness']))\n",
    "    \n",
    "    return pd.DataFrame(feature_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x = features(train_df, feature_names)\n",
    "train_y = train_df['expect_result']\n",
    "test_x = features(test_df, feature_names)\n",
    "test_y = test_df['expect_result']\n",
    "\n",
    "train_y[train_y == -1] = 0\n",
    "test_y[test_y == -1] = 0\n",
    "test_x = test_x.fillna(0)\n",
    "train_x = train_x.fillna(0)\n",
    "# test_df = pd.read_csv(data_root + '/' + 'test.csv')\n",
    "# test_x = features(test_df, feature_names)\n",
    "# test_y = test_df['expect_result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier as ada\n",
    "from sklearn.linear_model import LogisticRegression as lg\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "gdbtModel = GradientBoostingClassifier(learning_rate=0.1, n_estimators=100, max_depth=3, min_samples_split=3)\n",
    "gdbtModel.fit(train_x, train_y)\n",
    "pResult = gdbtModel.predict(test_x)\n",
    "print(classification_report(test_y, pResult))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "# joblib.dump(gdbtModel, '../production/model')\n",
    "# sum(pResult[-217:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(train_x, label=train_y)\n",
    "dtest = xgb.DMatrix(test_x, label=test_y)\n",
    "param = {'max_depth': 5, 'eta': 1, 'silent': 1, 'objective': 'binary:logistic', 'learning_rate':0.01, 'max_delta_step':1, 'subsample':0.8}\n",
    "param['nthread'] = 4\n",
    "param['eval_metric'] = 'auc'\n",
    "evallist = [(dtest, 'eval'), (dtrain, 'train')]\n",
    "\n",
    "num_round = 2000\n",
    "bst = xgb.train(param, dtrain, num_round, evallist)\n",
    "\n",
    "predict = bst.predict(dtest)\n",
    "# print(classification_report(test_y, pResult))\n",
    "result = list()\n",
    "for score in predict:\n",
    "    if score >= 0.5:\n",
    "        result.append(1)\n",
    "    else:\n",
    "        result.append(0)\n",
    "print(classification_report(test_y, result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.sum(result[-216:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ls_eval_df = pd.read_csv(data_root + '/' + 'ls_eval.csv')\n",
    "ls_eval_x = features(ls_eval_df, feature_names)\n",
    "ls_eval_y = ls_eval_df['expect_result']\n",
    "\n",
    "pResult = gdbtModel.predict(ls_eval_x)\n",
    "print(classification_report(ls_eval_y, pResult))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eval_df = pd.read_csv(data_root + '/' + 'eval_skew.csv')\n",
    "eval_df_x = features(eval_df, feature_names)\n",
    "eval_df_y = eval_df['expect_result']\n",
    "\n",
    "pResult = gdbtModel.predict(eval_df_x)\n",
    "print(classification_report(eval_df_y, pResult))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hard_eval_df = eval_df[eval_df.reason == 61]\n",
    "eval_df_x = features(hard_eval_df, feature_names)\n",
    "eval_df_y = hard_eval_df['expect_result']\n",
    "pResult = gdbtModel.predict(eval_df_x)\n",
    "print(classification_report(eval_df_y, pResult))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hard_eval_df = eval_df[eval_df.reason == 62]\n",
    "eval_df_x = features(hard_eval_df, feature_names)\n",
    "eval_df_y = hard_eval_df['expect_result']\n",
    "pResult = gdbtModel.predict(eval_df_x)\n",
    "print(classification_report(eval_df_y, pResult))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 开始斜边的波形调研，先看用现有的基线能得到多好的测试结果\n",
    "SKEW_DATA_FPATH='/Volumes/workspace/projects/signal_classification/data/特殊次品样本/斜角_严重.20190515/'\n",
    "data_reader = DataReader()\n",
    "skew_angel_df = data_reader.get_signal_list(SKEW_DATA_FPATH)\n",
    "skew_angel_df['expect_result'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = skew_angel_df.head()['case_path'][4]\n",
    "signals = pd.read_csv(path, skiprows=1)\n",
    "signals[200:220].plot()\n",
    "feas = sigMgr.get_features(path, request_param={'skip_row':[1], 'model_path':['train']})\n",
    "# medfiltered_signals = Filter.medfilter(normalized_signals, 9)\n",
    "print(np.mean(feas['unit_interviene_skewness']))\n",
    "print(np.std(feas['unit_interviene_skewness']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feas = sigMgr.get_features(path, request_param={'skip_row':[1], 'model_path':['train']})\n",
    "# medfiltered_signals = Filter.medfilter(normalized_signals, 9)\n",
    "print(np.mean(feas['unit_interviene_skewness']))\n",
    "print(np.std(feas['unit_interviene_skewness']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = normal_cases['case_path'].reset_index(drop=True)[6]\n",
    "feas = sigMgr.get_features(path, request_param={'skip_row':[1], 'model_path':['train']})\n",
    "normalized_signals = feas['normalized_signals'] \n",
    "medfiltered_signals = Filter.medfilter(normalized_signals, 5)\n",
    "pd.DataFrame(medfiltered_signals)[0:500].plot()\n",
    "print(np.mean(feas['unit_interviene_skewness']))\n",
    "print(np.std(feas['unit_interviene_skewness']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 开始斜边的波形调研，先看用现有的基线能得到多好的测试结果\n",
    "SKEW_DATA_FPATH='/Volumes/workspace/projects/signal_classification/data/特殊次品样本/斜角_严重.20190515/'\n",
    "data_reader = DataReader()\n",
    "skew_angel_df = data_reader.get_signal_list(SKEW_DATA_FPATH)\n",
    "skew_angel_df['expect_result'] = 1\n",
    "skew_unit_skewness_list = []\n",
    "for path in skew_angel_df['case_path']:\n",
    "    feas = sigMgr.get_features(path, request_param={'skip_row':[1], 'model_path':['train']})\n",
    "    skew_unit_skewness_list.extend(feas['unit_interviene_skewness'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (np.mean(skew_unit_skewness_list), np.std(skew_unit_skewness_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SKEW_DATA_FPATH_LIGHT='/Volumes/workspace/projects/signal_classification/data/特殊次品样本/斜角_轻微.20190515/'\n",
    "skew_light_angel_df = data_reader.get_signal_list(SKEW_DATA_FPATH_LIGHT)\n",
    "skew_light_angel_df['expect_result'] = 1\n",
    "skew_unit_skewness_list = []\n",
    "for path in skew_light_angel_df['case_path']:\n",
    "    feas = sigMgr.get_features(path, request_param={'skip_row':[1], 'model_path':['train']})\n",
    "    count = 0\n",
    "    for angel in feas['unit_interviene_skewness']:\n",
    "        if angel > (0.0124 + 1 * 0.035):\n",
    "            count+=1\n",
    "    skew_unit_skewness_list.append(count)\n",
    "#     skew_unit_skewness_list.extend(feas['unit_interviene_skewness'])\n",
    "print (np.mean(skew_unit_skewness_list), np.std(skew_unit_skewness_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "new_List = []\n",
    "for val in skew_unit_skewness_list:\n",
    "    if math.isnan(val) or math.isinf(val):\n",
    "        continue\n",
    "    new_List.append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (np.mean(new_List), np.std(new_List))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.distplot(new_List)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "goods = []\n",
    "for path in norm_df[norm_df.expect_result == 0]['case_path']:\n",
    "    feas = sigMgr.get_features(path, request_param={'skip_row':[1], 'model_path':['train']})\n",
    "    count = 0\n",
    "    for angel in feas['unit_interviene_skewness']:\n",
    "        if angel > (0.0124 + 1 * 0.035):\n",
    "            count+=1\n",
    "    goods.append(count)\n",
    "print (np.mean(goods), np.std(goods))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.distplot(new_List)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tpy2]",
   "language": "python",
   "name": "conda-env-tpy2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
